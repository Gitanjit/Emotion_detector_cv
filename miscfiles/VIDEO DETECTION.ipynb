{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"VIDEO DETECTION.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"E1CydAfQF1By"},"source":["# VIDEO EMOTION DETECTION\n"]},{"cell_type":"code","metadata":{"id":"a11K-hr2F1Bz"},"source":["import numpy as np\n","import argparse\n","import matplotlib.pyplot as plt\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Zi-ERyNlF1B2","outputId":"30057417-2767-4f83-f08c-6f6418757606"},"source":["conda info --envs"],"execution_count":null,"outputs":[{"output_type":"stream","text":["# conda environments:\n","#\n","base                  *  /opt/anaconda3\n","\n","\n","Note: you may need to restart the kernel to use updated packages.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SyMy0sJAF1B5"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Flatten\n","from tensorflow.keras.layers import Conv2D\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6iDtJvssF1B7"},"source":["from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.layers import MaxPooling2D\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YYy4i3IIF1B-"},"source":["import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PwnoR12yF1CA","outputId":"5f4ba9ba-e15c-435e-a1a5-b1cdaeeef086"},"source":["pip install opencv-python"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting opencv-python\n","  Using cached opencv-python-4.4.0.44.tar.gz (88.9 MB)\n","  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/lib/python3.8/site-packages (from opencv-python) (1.18.5)\n","Building wheels for collected packages: opencv-python\n","  Building wheel for opencv-python (PEP 517) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for opencv-python: filename=opencv_python-4.4.0.44-cp38-cp38-macosx_10_12_x86_64.whl size=25642685 sha256=b1db892dd3ca04c4ecf9ef12110e0388dd2b6ec318d1759c338e1d2117ff9a46\n","  Stored in directory: /Users/home/Library/Caches/pip/wheels/8f/2e/c4/3f1be5b275ea88b7ad1fb92991709bba0da241c06a6bca58a3\n","Successfully built opencv-python\n","Installing collected packages: opencv-python\n","Successfully installed opencv-python-4.4.0.44\n","Note: you may need to restart the kernel to use updated packages.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6YSHka0-F1CC"},"source":["import cv2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KpCmqNgAF1CF"},"source":["import sys, os  \n","import pandas as pd  \n","import numpy as np  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e9z1xBP_F1CH"},"source":["from keras.models import Sequential  \n","from keras.layers import Dense, Dropout, Activation, Flatten  \n","from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D  \n","from keras.losses import categorical_crossentropy  \n","from keras.optimizers import Adam  \n","from keras.regularizers import l2  \n","from keras.utils import np_utils  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJ11d5XgF1CJ"},"source":["# pd.set_option('display.max_rows', 500)  \n","# pd.set_option('display.max_columns', 500)  \n","# pd.set_option('display.width', 1000)  \n","  \n","df=pd.read_csv('fer2013.csv') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qEf74D0YF1CL","outputId":"97ba5dbf-7442-4d4e-cf3c-87fb12237179"},"source":["df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>emotion</th>\n","      <th>pixels</th>\n","      <th>Usage</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n","      <td>Training</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n","      <td>Training</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n","      <td>Training</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n","      <td>Training</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6</td>\n","      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n","      <td>Training</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>35882</th>\n","      <td>6</td>\n","      <td>50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...</td>\n","      <td>PrivateTest</td>\n","    </tr>\n","    <tr>\n","      <th>35883</th>\n","      <td>3</td>\n","      <td>178 174 172 173 181 188 191 194 196 199 200 20...</td>\n","      <td>PrivateTest</td>\n","    </tr>\n","    <tr>\n","      <th>35884</th>\n","      <td>0</td>\n","      <td>17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...</td>\n","      <td>PrivateTest</td>\n","    </tr>\n","    <tr>\n","      <th>35885</th>\n","      <td>3</td>\n","      <td>30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...</td>\n","      <td>PrivateTest</td>\n","    </tr>\n","    <tr>\n","      <th>35886</th>\n","      <td>2</td>\n","      <td>19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...</td>\n","      <td>PrivateTest</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>35887 rows Ã— 3 columns</p>\n","</div>"],"text/plain":["       emotion                                             pixels        Usage\n","0            0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...     Training\n","1            0  151 150 147 155 148 133 111 140 170 174 182 15...     Training\n","2            2  231 212 156 164 174 138 161 173 182 200 106 38...     Training\n","3            4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...     Training\n","4            6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...     Training\n","...        ...                                                ...          ...\n","35882        6  50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...  PrivateTest\n","35883        3  178 174 172 173 181 188 191 194 196 199 200 20...  PrivateTest\n","35884        0  17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...  PrivateTest\n","35885        3  30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...  PrivateTest\n","35886        2  19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...  PrivateTest\n","\n","[35887 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"P5mM1avmF1CO"},"source":["# print(df.info())  \n","# print(df[\"Usage\"].value_counts())  \n","  \n","# print(df.head())  \n","X_train,train_y,X_test,test_y=[],[],[],[]  \n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n1OL7k3KF1CP"},"source":["for index, row in df.iterrows():  \n","    val=row['pixels'].split(\" \")  \n","    try:  \n","        if 'Training' in row['Usage']:  \n","           X_train.append(np.array(val,'float32'))  \n","           train_y.append(row['emotion'])  \n","        elif 'PublicTest' in row['Usage']:  \n","           X_test.append(np.array(val,'float32'))  \n","           test_y.append(row['emotion'])  \n","    except:  \n","        print(f\"error occured at index :{index} and row:{row}\")  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SVSEVfnzF1CR"},"source":["num_features = 64  \n","num_labels = 7  \n","batch_size = 64  \n","epochs = 30  \n","width, height = 48, 48  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MVV9G0YoF1CU"},"source":["X_train = np.array(X_train,'float32')  \n","train_y = np.array(train_y,'float32')  \n","X_test = np.array(X_test,'float32')  \n","test_y = np.array(test_y,'float32')  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R8Cs0dnWF1CW"},"source":["train_y=np_utils.to_categorical(train_y, num_classes=num_labels)  \n","test_y=np_utils.to_categorical(test_y, num_classes=num_labels)  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W2bE-MGDF1CY"},"source":["#cannot produce  \n","#normalizing data between oand 1  \n","X_train -= np.mean(X_train, axis=0)  \n","X_train /= np.std(X_train, axis=0)  \n","  \n","X_test -= np.mean(X_test, axis=0)  \n","X_test /= np.std(X_test, axis=0)  \n","  \n","X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)  \n","  \n","X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"to1PdepxF1CZ"},"source":["# print(f\"shape:{X_train.shape}\")  \n","##designing the cnn  \n","#1st convolution layer  \n","model = Sequential()  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1N_nIQVAF1Cb"},"source":["model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1:])))  \n","model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))  \n","# model.add(BatchNormalization())  \n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))  \n","model.add(Dropout(0.5))  \n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uG9VtREvF1Cd"},"source":["#2nd convolution layer  \n","model.add(Conv2D(64, (3, 3), activation='relu'))  \n","model.add(Conv2D(64, (3, 3), activation='relu'))  \n","# model.add(BatchNormalization())  \n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))  \n","model.add(Dropout(0.5))  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XgLIbqDQF1Cg"},"source":["#3rd convolution layer  \n","model.add(Conv2D(128, (3, 3), activation='relu'))  \n","model.add(Conv2D(128, (3, 3), activation='relu'))  \n","# model.add(BatchNormalization())  \n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))  \n","  \n","model.add(Flatten())  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Mxrdm-RF1Cj"},"source":["#fully connected neural networks  \n","model.add(Dense(1024, activation='relu'))  \n","model.add(Dropout(0.2))  \n","model.add(Dense(1024, activation='relu'))  \n","model.add(Dropout(0.2))  \n","  \n","model.add(Dense(num_labels, activation='softmax'))  \n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v3lA1jiIF1Ck"},"source":["#Compliling the model  \n","model.compile(loss=categorical_crossentropy,  \n","              optimizer=Adam(),  \n","              metrics=['accuracy'])  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HYjqwSByF1Cm","outputId":"2ae0cd59-5ff5-4197-c51a-e0ad496a0716"},"source":["model.fit(X_train, train_y,  \n","          batch_size=batch_size,  \n","          epochs=epochs,  \n","          verbose=1,  \n","          validation_data=(X_test, test_y),  \n","          shuffle=True)  \n","  "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","449/449 [==============================] - 917s 2s/step - loss: 1.7272 - accuracy: 0.2924 - val_loss: 1.5372 - val_accuracy: 0.3845\n","Epoch 2/30\n","449/449 [==============================] - 1023s 2s/step - loss: 1.5098 - accuracy: 0.4059 - val_loss: 1.4034 - val_accuracy: 0.4611\n","Epoch 3/30\n","449/449 [==============================] - 725s 2s/step - loss: 1.4074 - accuracy: 0.4545 - val_loss: 1.3195 - val_accuracy: 0.4837\n","Epoch 4/30\n","449/449 [==============================] - 740s 2s/step - loss: 1.3453 - accuracy: 0.4780 - val_loss: 1.3127 - val_accuracy: 0.4834\n","Epoch 5/30\n","449/449 [==============================] - 746s 2s/step - loss: 1.2990 - accuracy: 0.4976 - val_loss: 1.2874 - val_accuracy: 0.4999\n","Epoch 6/30\n","449/449 [==============================] - 578s 1s/step - loss: 1.2564 - accuracy: 0.5172 - val_loss: 1.2614 - val_accuracy: 0.5249\n","Epoch 7/30\n","449/449 [==============================] - 1238s 3s/step - loss: 1.2295 - accuracy: 0.5264 - val_loss: 1.1956 - val_accuracy: 0.5325\n","Epoch 8/30\n","449/449 [==============================] - 538s 1s/step - loss: 1.2040 - accuracy: 0.5375 - val_loss: 1.2012 - val_accuracy: 0.5397\n","Epoch 9/30\n","449/449 [==============================] - 536s 1s/step - loss: 1.1794 - accuracy: 0.5463 - val_loss: 1.1857 - val_accuracy: 0.5539\n","Epoch 10/30\n","449/449 [==============================] - 2167s 5s/step - loss: 1.1573 - accuracy: 0.5556 - val_loss: 1.1847 - val_accuracy: 0.5422\n","Epoch 11/30\n","449/449 [==============================] - 538s 1s/step - loss: 1.1336 - accuracy: 0.5669 - val_loss: 1.1831 - val_accuracy: 0.5511\n","Epoch 12/30\n","449/449 [==============================] - 613s 1s/step - loss: 1.1204 - accuracy: 0.5704 - val_loss: 1.1642 - val_accuracy: 0.5556\n","Epoch 13/30\n","449/449 [==============================] - 3269s 7s/step - loss: 1.0958 - accuracy: 0.5807 - val_loss: 1.1887 - val_accuracy: 0.5522\n","Epoch 14/30\n","449/449 [==============================] - 954s 2s/step - loss: 1.0843 - accuracy: 0.5837 - val_loss: 1.1647 - val_accuracy: 0.5589\n","Epoch 15/30\n","449/449 [==============================] - 713s 2s/step - loss: 1.0653 - accuracy: 0.5908 - val_loss: 1.1566 - val_accuracy: 0.5617\n","Epoch 16/30\n","449/449 [==============================] - 747s 2s/step - loss: 1.0440 - accuracy: 0.5999 - val_loss: 1.1832 - val_accuracy: 0.5542\n","Epoch 17/30\n","449/449 [==============================] - 752s 2s/step - loss: 1.0290 - accuracy: 0.6069 - val_loss: 1.1771 - val_accuracy: 0.5573\n","Epoch 18/30\n","449/449 [==============================] - 718s 2s/step - loss: 1.0127 - accuracy: 0.6127 - val_loss: 1.1468 - val_accuracy: 0.5687\n","Epoch 19/30\n","449/449 [==============================] - 1033s 2s/step - loss: 1.0047 - accuracy: 0.6163 - val_loss: 1.1444 - val_accuracy: 0.5765\n","Epoch 20/30\n","449/449 [==============================] - 781s 2s/step - loss: 0.9844 - accuracy: 0.6220 - val_loss: 1.1741 - val_accuracy: 0.5620\n","Epoch 21/30\n","449/449 [==============================] - 759s 2s/step - loss: 0.9689 - accuracy: 0.6280 - val_loss: 1.1648 - val_accuracy: 0.5653\n","Epoch 22/30\n","449/449 [==============================] - 677s 2s/step - loss: 0.9541 - accuracy: 0.6372 - val_loss: 1.1709 - val_accuracy: 0.5743\n","Epoch 23/30\n","449/449 [==============================] - 760s 2s/step - loss: 0.9332 - accuracy: 0.6451 - val_loss: 1.1746 - val_accuracy: 0.5726\n","Epoch 24/30\n","449/449 [==============================] - 543s 1s/step - loss: 0.9303 - accuracy: 0.6438 - val_loss: 1.1911 - val_accuracy: 0.5698\n","Epoch 25/30\n","449/449 [==============================] - 625s 1s/step - loss: 0.9156 - accuracy: 0.6495 - val_loss: 1.1977 - val_accuracy: 0.5620\n","Epoch 26/30\n","449/449 [==============================] - 549s 1s/step - loss: 0.8994 - accuracy: 0.6554 - val_loss: 1.2118 - val_accuracy: 0.5550\n","Epoch 27/30\n","449/449 [==============================] - 541s 1s/step - loss: 0.8919 - accuracy: 0.6633 - val_loss: 1.1881 - val_accuracy: 0.5645\n","Epoch 28/30\n","449/449 [==============================] - 542s 1s/step - loss: 0.8780 - accuracy: 0.6666 - val_loss: 1.2121 - val_accuracy: 0.5678\n","Epoch 29/30\n","449/449 [==============================] - 542s 1s/step - loss: 0.8646 - accuracy: 0.6717 - val_loss: 1.2119 - val_accuracy: 0.5600\n","Epoch 30/30\n","449/449 [==============================] - 559s 1s/step - loss: 0.8586 - accuracy: 0.6779 - val_loss: 1.2009 - val_accuracy: 0.5687\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x11b31f040>"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"6jLJdAGgF1Co"},"source":["#Saving the  model to  use it later on  \n","fer_json = model.to_json()  \n","with open(\"fer.json\", \"w\") as json_file:  \n","    json_file.write(fer_json)  \n","model.save_weights(\"fer.h5\")  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mkqDR3CtF1Cq"},"source":["import os  \n","import cv2  \n","import numpy as np  \n","from keras.models import model_from_json  \n","from keras.preprocessing import image  \n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ux_9CZCuF1Cu"},"source":["  \n","#load model  \n","model = model_from_json(open(\"fer.json\", \"r\").read())  \n","#load weights  \n","model.load_weights('fer.h5')  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aUja0jbeF1Cw"},"source":["face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"drvGRxi2F1Cy"},"source":["cap=cv2.VideoCapture(0)  \n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"18PFWrXKF1Cz","outputId":"273bd9fd-74ac-4368-9fa6-b4dcc28d8d4f"},"source":["while True:  \n","    ret,test_img=cap.read()# captures frame and returns boolean value and captured image  \n","    if not ret:  \n","        continue  \n","    gray_img= cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)  \n","  \n","    faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5) \n","    \n","    for (x,y,w,h) in faces_detected:  \n","        cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=7)  \n","        roi_gray=gray_img[y:y+w,x:x+h]#cropping region of interest i.e. face area from  image  \n","        roi_gray=cv2.resize(roi_gray,(48,48))  \n","        img_pixels = image.img_to_array(roi_gray)  \n","        img_pixels = np.expand_dims(img_pixels, axis = 0)  \n","        img_pixels /= 255  \n","  \n","        predictions = model.predict(img_pixels)  \n","  \n","        #find max indexed array  \n","        max_index = np.argmax(predictions[0])  \n","  \n","        emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')  \n","        predicted_emotion = emotions[max_index]  \n","  \n","        cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)  \n","  \n","    resized_img = cv2.resize(test_img, (1000, 700))  \n","    cv2.imshow('Facial emotion analysis ',resized_img)  \n","  \n","  \n","  \n","    if cv2.waitKey(10) == ord('q'):#wait until 'q' key is pressed  \n","        break  \n","  \n","cap.release()  \n","cv2.destroyAllWindows  "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function destroyAllWindows>"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"KHl69ZtMF1C1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zIsYbmOWF1C3"},"source":[""],"execution_count":null,"outputs":[]}]}